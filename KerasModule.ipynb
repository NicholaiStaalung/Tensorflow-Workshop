{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This workshop will introduce a utilization of the Keras open source module with a Tensorflow backend.\n",
    "Keras has become popular as an alternative to pure Tensorflow beacuse of its easy implementation, and similirity in implentation of other popular open source ML libraries. However, the downside to Keras is that it is significantly slower than pure Tensorflow. We will illustrate an approach to a regression problem using a Deep Neural Network, and describing each step along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this workshop is a webscraped dataset from a danish online auction house containing product information. The data has been preprocessed for you convenience, so focus can be on the Machine Learning. The features in question are the sales price (price), the presale valuation (valuation), a procut title (titles) and 106 binary features of the product category. We will try to predict the sales price of the products based on the valuation and the product category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 123 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random seed is important for reproducability when running on a CPU - However it's indifferent when running on the GPU as a GPU sets a number of random seeds creating a challenge for reproduction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('input/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data consists of many features having zeros in it. These are binary features. Check for yourself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check some of the product titles, prices and valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[['titles', 'price', 'valuation']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete the titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data['titles']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont need the titles of the product in this application, as we s are simply trying to predict prices based on valuation and category"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get a feel for the distribution of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=data[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=data[\"valuation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the products are realtively low priced products, and then there are som outliers. Maybe we could restrain the input to products only in a smaller range of prices. That also means we would only build an application for relatively low priced products"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restraining input based on valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ir = data[data['valuation'] <= 25000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are removing all products that have a larger valuation than the defined amount. You are welcome to play around with it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check the data again to see the impact of restraining the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ir.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=data_ir[\"price\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.violinplot(x=data_ir[\"valuation\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the picture tell you? If you have restrained input like me on 10.000 DKK for valuations, then there are som discrepenacy between the valuation and the sales price. It tells us that some products a great dela lower than the actual sales price turned out to be. So is the valuation really a good predictor for sales prices at all?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the correlation coefficient between price and valuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.corrcoef(data_ir[['price', 'valuation']], rowvar=False)[0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty high correlation of 67 pct. This serves as evidence that the valuation turns out to be a great predictor. Keep in mind that this will probably change as you change the input restrictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How about the distribution of the product categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Neural nets you tend to like that each instance is represented more than a few times in order to substantiate a pattern. Therefore we should delete product categories if they are represented less than 10 times just to be sure. Also this is a hyperparameter as well. You can play with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data_ir.columns:\n",
    "    if data_ir[col].sum() < 10:\n",
    "        print('Removing column %s that only occurs %i times' %(col, data_ir[col].sum()))\n",
    "        del data_ir[col]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data for Cross validation purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(data_ir, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are splliting the data in order to validate the model based on unseen data or test data. We split the data into a training and a test set for now. Keras will independently split the training data into a training and validation set. That means we train the model on the training data, optimize the model weights on the validation data, and finally test the model on the test data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Keras NN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initlizing the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are familiar with tensorflow then you have probably heard the word 'Graph' a few times. Sequential is like initliazing this graph. Its basically like defining something with an empty piece of paper. Its has restrains in the form of edges, but it contains nothing. The graph still doesn't know how many input features we add to it, the structure of the hidden layers or how many dimensions the output layer will have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a hidden layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodesHidden1 = int((len(train.drop('price', axis=1).T) + 1 ) / 10)\n",
    "graph.add(Dense(units=nodesHidden1, kernel_initializer=\"normal\", \\\n",
    "#                 kernel_regularizer=regularizers.l2(0.01), \\\n",
    "#                 activity_regularizer=regularizers.l1(0.01), \\\n",
    "                activation='relu', input_shape = (len(train.drop('price', axis=1).T), ))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we add layers to our graph, then we draw connections between each layer node. These connections are often referred to as weights. When adding a layer we need to define layer dimensions (nodes or neurons), the activation function and what the dimensions of the prior layer is. Play around with the activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding dropout to prevet overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop_rate = 0.25\n",
    "# graph.add(Dropout(drop_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropout is a way to prevent overfitting. It randomly drops nodes/neurons from the graph, so the graph wont generalize to the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.add(Dense(units=1, kernel_initializer='normal'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are dealing with a one-dimensiaonal output, which is the case in regression problems, then we only define one output dimension. Had we been dealing with a classification problem we would define the dimensions by number of categories minus 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compiling the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.compile(optimizer='adam', loss='mean_squared_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling basically tells the graph to close for any new layers, defines the optimizing algorithm and defines the metric in which we want to measure our results. We are using ADAM optimizer, but without defining the parameters of the optimization. But you can play around with this as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "earlystop = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=3, verbose=1, mode='auto')\n",
    "callbacks_list = [earlystop] #never figured out why it needs to be changed to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use early stopping because we dont want to keep training our graph whn there are no advantage to gain. The import keyword here is 'patience'. It tells the graph how many times it should train without improving before stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When fitting the model, then it takes the input, the ouput and a the batch (number of rows) that we want to train on at a time. Th epocs are the maximum number of training iterations, and the validion split defines the validation set size. Remember the vcalidation is for measuring improvements during each training iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingHist = graph.fit(train.drop('price', axis=1).values, train['price'].values, \\\n",
    "          batch_size=50, epochs=100, callbacks=callbacks_list, validation_split=0.1)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(graph, to_file='graph.png', show_shapes=True)\n",
    "Image('graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainingHist.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(trainingHist.history['loss'])\n",
    "plt.plot(trainingHist.history['val_loss'])\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Measuring the performance of the graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we calculate the predictions based on the training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test = graph.predict(test.drop('price', axis=1).values)\n",
    "y_pred_train = graph.predict(train.drop('price', axis=1).values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, we calculate some scoring statistics. We use root mean squared error and the sum of explained squared errors (R²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meanPriceArray = np.ones(len(train)) * train['price'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmseBenchmark = np.sqrt(mean_squared_error(y_true = train['price'].values, y_pred = meanPriceArray))\n",
    "rmseTrain = np.sqrt(mean_squared_error(y_true = train['price'].values, y_pred = y_pred_train))\n",
    "rmseTest = np.sqrt(mean_squared_error(y_true = test['price'].values, y_pred = y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuraciesBenchmark = r2_score(y_true = train['price'].values, y_pred = meanPriceArray)\n",
    "accuraciesTest = r2_score(y_true = test['price'].values, y_pred = y_pred_test)\n",
    "accuraciesTrain = r2_score(y_true = train['price'].values, y_pred = y_pred_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we print the scores to check performance and the fitting of the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n------------------------------------')\n",
    "print('\\nAccuracy benchmark: ', round(accuraciesBenchmark, 2))\n",
    "print('\\nRMSE benchmark: ', round(rmseBenchmark, 2))\n",
    "print('\\n------------------------------------')\n",
    "print('\\nAccuracy train: ', round(accuraciesTrain,2))\n",
    "print('\\nRMSE train: ', round(rmseTrain, 2))\n",
    "print('\\n------------------------------------')\n",
    "print('\\nAccuracy test: ', round(accuraciesTest, 2))\n",
    "print('\\nRMSE test: ', round(rmseTest, 2))\n",
    "print('\\n------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we do? And can you improve our results? How would you go about dealing with the overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "TF-Workshop",
   "language": "python",
   "name": "tf-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
